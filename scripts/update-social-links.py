#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Sosyal Medya Linklerini Web Sitesinden Ã‡eken Script
Web sitesinden sosyal medya linklerini bulup JSON dosyasÄ±nÄ± gÃ¼nceller.
"""

import json
import re
import sys
from pathlib import Path
from urllib.parse import urlparse, urljoin
import requests
from bs4 import BeautifulSoup

# Sosyal medya platformlarÄ± ve pattern'leri
SOCIAL_PATTERNS = {
    'linkedin': [
        r'linkedin\.com/company/([^/\s"\']+)',
        r'linkedin\.com/in/([^/\s"\']+)',
        r'tr\.linkedin\.com/company/([^/\s"\']+)'
    ],
    'x': [
        r'twitter\.com/([^/\s"\']+)',
        r'x\.com/([^/\s"\']+)'
    ],
    'instagram': [
        r'instagram\.com/([^/\s"\']+)'
    ],
    'facebook': [
        r'facebook\.com/([^/\s"\']+)',
        r'fb\.com/([^/\s"\']+)'
    ],
    'youtube': [
        r'youtube\.com/c/([^/\s"\']+)',
        r'youtube\.com/channel/([^/\s"\']+)',
        r'youtube\.com/user/([^/\s"\']+)',
        r'youtube\.com/@([^/\s"\']+)'
    ],
    'github': [
        r'github\.com/([^/\s"\']+)'
    ]
}

def fetch_html(url, timeout=20, verify_ssl=False):
    """Web sitesinden HTML iÃ§eriÄŸini Ã§eker"""
    try:
        # URL'yi parse et
        from urllib.parse import urlparse
        parsed = urlparse(url)
        base_domain = f"{parsed.scheme}://{parsed.netloc}"

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',  # BazÄ± siteler keep-alive bekliyor
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            'Referer': base_domain
        }

        # SSL uyarÄ±sÄ±nÄ± bastÄ±r
        import urllib3
        if not verify_ssl:
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

        # Daha toleranslÄ± HTTPAdapter
        session = requests.Session()
        adapter = requests.adapters.HTTPAdapter(
            pool_connections=10,
            pool_maxsize=10,
            max_retries=1
        )
        session.mount('http://', adapter)
        session.mount('https://', adapter)
        session.headers.update(headers)

        response = session.get(url, timeout=timeout, allow_redirects=True, verify=verify_ssl)

        # 500 hatasÄ± olsa bile HTML varsa kullan (bazÄ± siteler hata verse de iÃ§erik dÃ¶ndÃ¼rÃ¼r)
        response.encoding = response.apparent_encoding
        html_content = response.text

        # Session'Ä± kapat
        session.close()

        # HTML iÃ§eriÄŸi varsa baÅŸarÄ±lÄ± sayalÄ±m (status code'a bakmadan)
        if html_content and len(html_content) > 1000:  # En az 1KB HTML olmalÄ±
            if response.status_code != 200:
                print(f"âš ï¸  HTTP {response.status_code} ancak HTML iÃ§eriÄŸi alÄ±ndÄ±")
            return html_content

        # HTML yoksa veya Ã§ok kÃ¼Ã§Ã¼kse hata ver
        response.raise_for_status()
        return html_content

    except requests.RequestException as e:
        print(f"âŒ HTML Ã§ekme hatasÄ±: {e}")
        return None

def follow_redirect(url, timeout=10):
    """Redirect linkini takip edip gerÃ§ek URL'i dÃ¶ndÃ¼rÃ¼r"""
    try:
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True, verify=False)
        return response.url
    except:
        return url

def extract_social_links(html, base_url):
    """HTML'den sosyal medya linklerini Ã§Ä±karÄ±r"""
    soup = BeautifulSoup(html, 'html.parser')
    social_links = {
        'linkedin': '',
        'x': '',
        'instagram': '',
        'facebook': '',
        'youtube': '',
        'github': ''
    }

    # TÃ¼m linkleri bul
    all_links = soup.find_all('a', href=True)

    # Redirect linkleri iÃ§in kontrol (/link/ gibi)
    redirect_links = {}
    for link in all_links:
        href = link.get('href', '')
        href_lower = href.lower()

        # /link/ redirect pattern'lerini kontrol et
        if '/link/' in href_lower:
            if 'linkedin' in href_lower:
                redirect_links['linkedin'] = urljoin(base_url, href)
            elif 'facebook' in href_lower or 'fb' in href_lower:
                redirect_links['facebook'] = urljoin(base_url, href)
            elif 'twitter' in href_lower or href_lower.endswith('/link/x'):
                redirect_links['x'] = urljoin(base_url, href)
            elif 'instagram' in href_lower:
                redirect_links['instagram'] = urljoin(base_url, href)
            elif 'youtube' in href_lower:
                redirect_links['youtube'] = urljoin(base_url, href)
            elif 'github' in href_lower:
                redirect_links['github'] = urljoin(base_url, href)

    for link in all_links:
        href = link.get('href', '')

        # Relative URL'leri absolute yap
        if href.startswith('/'):
            href = urljoin(base_url, href)

        # Her sosyal medya platformu iÃ§in kontrol et
        for platform, patterns in SOCIAL_PATTERNS.items():
            if social_links[platform]:  # Zaten bulunduysa atla
                continue

            for pattern in patterns:
                match = re.search(pattern, href, re.IGNORECASE)
                if match:
                    # Temiz URL oluÅŸtur
                    if platform == 'linkedin':
                        if 'company' in href:
                            social_links[platform] = f"https://linkedin.com/company/{match.group(1)}"
                        else:
                            social_links[platform] = f"https://linkedin.com/in/{match.group(1)}"
                    elif platform == 'x':
                        social_links[platform] = f"https://x.com/{match.group(1)}"
                    elif platform == 'instagram':
                        social_links[platform] = f"https://instagram.com/{match.group(1)}"
                    elif platform == 'facebook':
                        social_links[platform] = f"https://facebook.com/{match.group(1)}"
                    elif platform == 'youtube':
                        # YouTube URL formatÄ±nÄ± koru
                        if 'channel' in href:
                            social_links[platform] = f"https://youtube.com/channel/{match.group(1)}"
                        elif 'user' in href:
                            social_links[platform] = f"https://youtube.com/user/{match.group(1)}"
                        elif '@' in href:
                            social_links[platform] = f"https://youtube.com/@{match.group(1)}"
                        else:
                            social_links[platform] = f"https://youtube.com/c/{match.group(1)}"
                    elif platform == 'github':
                        social_links[platform] = f"https://github.com/{match.group(1)}"

                    print(f"âœ… {platform.capitalize()}: {social_links[platform]}")
                    break

    # Redirect linkleri varsa takip et
    if redirect_links:
        print(f"\nğŸ”„ {len(redirect_links)} redirect linki bulundu, takip ediliyor...")
        for platform, redirect_url in redirect_links.items():
            if not social_links[platform]:  # HenÃ¼z bulunmadÄ±ysa
                try:
                    real_url = follow_redirect(redirect_url, timeout=10)
                    social_links[platform] = real_url
                    print(f"âœ… {platform.capitalize()} (redirect): {real_url}")
                except Exception as e:
                    print(f"âš ï¸  {platform.capitalize()} redirect hatasÄ±: {e}")

    return social_links

def update_company_json(json_path, social_links, dry_run=False):
    """JSON dosyasÄ±nÄ± sosyal medya linkleriyle gÃ¼nceller"""
    try:
        # JSON dosyasÄ±nÄ± oku
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # Mevcut deÄŸerleri sakla
        old_social = data.get('social', {}).copy()

        # Yeni deÄŸerleri gÃ¼ncelle (sadece boÅŸ olanlarÄ± - mevcut linklere DOKUNMA)
        updated = False
        for platform, link in social_links.items():
            if link and not old_social.get(platform):
                data['social'][platform] = link
                updated = True
                print(f"  ğŸ“ {platform} gÃ¼ncellendi: {link}")
            elif link and old_social.get(platform):
                # Mevcut link varsa atla - DOKUNMA
                print(f"  âœ“ {platform} zaten var, korunuyor: {old_social.get(platform)}")

        if not updated:
            print("  â„¹ï¸  GÃ¼ncellenecek yeni link bulunamadÄ±")
            return False

        # DosyayÄ± kaydet
        if not dry_run:
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"âœ… Dosya gÃ¼ncellendi: {json_path}")
        else:
            print(f"ğŸ” DRY RUN: Dosya gÃ¼ncellenmedi (--dry-run)")

        return True

    except Exception as e:
        print(f"âŒ JSON gÃ¼ncelleme hatasÄ±: {e}")
        return False

def main():
    import argparse

    parser = argparse.ArgumentParser(description='Sosyal medya linklerini web sitesinden Ã§eker ve JSON dosyasÄ±nÄ± gÃ¼nceller')
    parser.add_argument('json_file', help='GÃ¼ncellenecek JSON dosyasÄ± (Ã¶rn: akgun.json)')
    parser.add_argument('--dry-run', action='store_true', help='Sadece gÃ¶ster, dosyayÄ± gÃ¼ncelleme')
    parser.add_argument('--timeout', type=int, default=10, help='HTTP timeout (saniye)')
    parser.add_argument('--verify-ssl', action='store_true', help='SSL sertifikasÄ±nÄ± doÄŸrula')

    args = parser.parse_args()

    # JSON dosyasÄ±nÄ± bul
    json_path = Path(args.json_file)
    if not json_path.exists():
        # public/data/company/ iÃ§inde ara
        alt_path = Path('public/data/company') / json_path.name
        if alt_path.exists():
            json_path = alt_path
        else:
            print(f"âŒ Dosya bulunamadÄ±: {args.json_file}")
            return 1

    print(f"ğŸ“‚ Dosya: {json_path}")

    # JSON dosyasÄ±nÄ± oku
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"âŒ JSON okuma hatasÄ±: {e}")
        return 1

    # Web adresini al
    web_url = data.get('contact', {}).get('web', '')
    if not web_url:
        print("âŒ Web adresi bulunamadÄ± (contact.web)")
        return 1

    print(f"ğŸŒ Web Sitesi: {web_url}")
    print(f"\nğŸ” Sosyal medya linkleri aranÄ±yor...\n")

    # HTML'i Ã§ek
    html = fetch_html(web_url, timeout=args.timeout, verify_ssl=args.verify_ssl)
    if not html:
        return 1

    # Sosyal medya linklerini Ã§Ä±kar
    social_links = extract_social_links(html, web_url)

    # Bulunan linkleri gÃ¶ster
    found_count = sum(1 for link in social_links.values() if link)
    print(f"\nğŸ“Š Toplam {found_count} sosyal medya linki bulundu\n")

    if found_count == 0:
        print("â„¹ï¸  HiÃ§ sosyal medya linki bulunamadÄ±")
        return 0

    # JSON'u gÃ¼ncelle
    print("ğŸ“ JSON dosyasÄ± gÃ¼ncelleniyor...\n")
    success = update_company_json(json_path, social_links, dry_run=args.dry_run)

    return 0 if success else 1

if __name__ == '__main__':
    sys.exit(main())
