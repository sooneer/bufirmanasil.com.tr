#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LinkedIn Firma Bilgisi Ã‡ekme Script (Remote Debugging)
Opera'da aÃ§Ä±k olan tarayÄ±cÄ± oturumunu kullanÄ±r.
"""

import json
import re
import sys
import time
import argparse
from pathlib import Path
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

# Windows terminal iÃ§in encoding ayarÄ±
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')


def connect_to_remote_browser(debugger_address='localhost:9222'):
    """AÃ§Ä±k olan Opera tarayÄ±cÄ±sÄ±na baÄŸlan"""
    print(f"ğŸ”— Remote tarayÄ±cÄ±ya baÄŸlanÄ±lÄ±yor: {debugger_address}")

    chrome_options = Options()
    chrome_options.add_experimental_option("debuggerAddress", debugger_address)

    # WebDriver'Ä± baÅŸlat
    try:
        driver = webdriver.Chrome(
            service=Service(ChromeDriverManager().install()),
            options=chrome_options
        )
        print("âœ… TarayÄ±cÄ±ya baÄŸlandÄ±!")
        return driver
    except Exception as e:
        print(f"âŒ TarayÄ±cÄ±ya baÄŸlanÄ±rken hata: {e}")
        print("\nğŸ’¡ Opera'yÄ± ÅŸu komutla baÅŸlatÄ±n:")
        print('   "C:\\Users\\soner.acar\\AppData\\Local\\Programs\\Opera\\opera.exe" --remote-debugging-port=9222 --user-data-dir="C:\\Users\\soner.acar\\AppData\\Roaming\\Opera Software\\Opera Stable"')
        return None


def extract_linkedin_info_from_page(driver, linkedin_url, timeout=30):
    """LinkedIn sayfasÄ±ndan bilgileri Ã§Ä±kar"""
    try:
        print(f"ğŸ” LinkedIn sayfasÄ± aÃ§Ä±lÄ±yor: {linkedin_url}")
        driver.get(linkedin_url)

        # SayfanÄ±n yÃ¼klenmesini bekle
        time.sleep(3)

        # Auth wall kontrolÃ¼
        if 'authwall' in driver.current_url.lower() or 'login' in driver.current_url.lower():
            print("âš ï¸  LinkedIn giriÅŸ ekranÄ±na yÃ¶nlendirildi!")
            print("ğŸ’¡ LÃ¼tfen Opera'da LinkedIn'e giriÅŸ yapÄ±n ve scripti tekrar Ã§alÄ±ÅŸtÄ±rÄ±n.")
            return None

        # Sayfa HTML'ini al
        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')

        info = {
            'name': '',
            'about': '',
            'tagline': '',
            'industry': '',
            'company_size': '',
            'headquarters': '',
            'website': '',
            'founded': ''
        }

        # Firma adÄ±
        try:
            # h1 baÅŸlÄ±ktan firma adÄ±nÄ± al
            name_elem = driver.find_element(By.CSS_SELECTOR, 'h1.org-top-card-summary__title')
            info['name'] = name_elem.text.strip()
            print(f"  âœ“ Name: {info['name']}")
        except NoSuchElementException:
            # Alternatif selector'lar
            try:
                name_elem = driver.find_element(By.CSS_SELECTOR, 'h1')
                info['name'] = name_elem.text.strip()
            except:
                pass

        # Tagline
        try:
            tagline_elem = driver.find_element(By.CSS_SELECTOR, 'p.org-top-card-summary__tagline')
            info['tagline'] = tagline_elem.text.strip()
            print(f"  âœ“ Tagline: {info['tagline']}")
        except NoSuchElementException:
            pass

        # About
        try:
            about_elem = driver.find_element(By.CSS_SELECTOR, 'p.break-words')
            info['about'] = about_elem.text.strip()
            print(f"  âœ“ About: {info['about'][:100]}...")
        except NoSuchElementException:
            # Alternatif: section iÃ§inden about'u bul
            try:
                about_section = driver.find_element(By.CSS_SELECTOR, 'section[data-test-id="about-us"]')
                paragraphs = about_section.find_elements(By.TAG_NAME, 'p')
                if paragraphs:
                    info['about'] = '\n'.join([p.text.strip() for p in paragraphs if p.text.strip()])
                    print(f"  âœ“ About: {info['about'][:100]}...")
            except:
                pass

        # Industry (SektÃ¶r)
        try:
            # "SektÃ¶r" veya "Industry" yazÄ±sÄ±nÄ± iÃ§eren dt elementi bul
            labels = driver.find_elements(By.CSS_SELECTOR, 'dt.org-page-details__definition-term')
            for label in labels:
                if 'Industry' in label.text or 'SektÃ¶r' in label.text:
                    # Hemen sonraki dd elementini al
                    industry_elem = label.find_element(By.XPATH, './following-sibling::dd')
                    info['industry'] = industry_elem.text.strip()
                    print(f"  âœ“ Industry: {info['industry']}")
                    break
        except:
            pass

        # Company size (Ã‡alÄ±ÅŸan sayÄ±sÄ±)
        try:
            labels = driver.find_elements(By.CSS_SELECTOR, 'dt.org-page-details__definition-term')
            for label in labels:
                if 'Company size' in label.text or 'Åirket bÃ¼yÃ¼klÃ¼ÄŸÃ¼' in label.text:
                    size_elem = label.find_element(By.XPATH, './following-sibling::dd')
                    info['company_size'] = size_elem.text.strip()
                    print(f"  âœ“ Company Size: {info['company_size']}")
                    break
        except:
            pass

        # Headquarters (Genel merkez)
        try:
            labels = driver.find_elements(By.CSS_SELECTOR, 'dt.org-page-details__definition-term')
            for label in labels:
                if 'Headquarters' in label.text or 'Genel merkez' in label.text:
                    hq_elem = label.find_element(By.XPATH, './following-sibling::dd')
                    info['headquarters'] = hq_elem.text.strip()
                    print(f"  âœ“ Headquarters: {info['headquarters']}")
                    break
        except:
            pass

        # Website
        try:
            labels = driver.find_elements(By.CSS_SELECTOR, 'dt.org-page-details__definition-term')
            for label in labels:
                if 'Website' in label.text or 'Web sitesi' in label.text:
                    web_elem = label.find_element(By.XPATH, './following-sibling::dd')
                    info['website'] = web_elem.text.strip()
                    print(f"  âœ“ Website: {info['website']}")
                    break
        except:
            pass

        # Founded (KuruluÅŸ yÄ±lÄ±)
        try:
            labels = driver.find_elements(By.CSS_SELECTOR, 'dt.org-page-details__definition-term')
            for label in labels:
                if 'Founded' in label.text or 'KuruluÅŸ' in label.text:
                    founded_elem = label.find_element(By.XPATH, './following-sibling::dd')
                    info['founded'] = founded_elem.text.strip()
                    print(f"  âœ“ Founded: {info['founded']}")
                    break
        except:
            pass

        return info

    except Exception as e:
        print(f"âŒ LinkedIn bilgi Ã§ekme hatasÄ±: {e}")
        import traceback
        traceback.print_exc()
        return None


def update_company_json_with_linkedin(json_path, linkedin_info, dry_run=False, force=False):
    """JSON dosyasÄ±nÄ± LinkedIn bilgileriyle gÃ¼nceller"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        updated = False
        updates = []

        # Name
        if linkedin_info.get('name') and (not data.get('name') or force):
            data['name'] = linkedin_info['name']
            updated = True
            updates.append(f"Name: {linkedin_info['name']}")

        # About
        if linkedin_info.get('about') and (not data.get('about') or force):
            data['about'] = linkedin_info['about']
            updated = True
            updates.append(f"About: {linkedin_info['about'][:100]}...")

        # Tagline
        if linkedin_info.get('tagline') and (not data.get('tagline') or force):
            data['tagline'] = linkedin_info['tagline']
            updated = True
            updates.append(f"Tagline: {linkedin_info['tagline']}")

        if not updated:
            print("  â„¹ï¸  GÃ¼ncellenecek yeni bilgi bulunamadÄ±")
            return False

        print("\nğŸ“ GÃ¼ncellenecek alanlar:")
        for update in updates:
            print(f"  â€¢ {update}")

        if not dry_run:
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"\nâœ… Dosya gÃ¼ncellendi: {json_path}")
        else:
            print(f"\nğŸ” DRY RUN: Dosya gÃ¼ncellenmedi")

        return True

    except Exception as e:
        print(f"âŒ JSON gÃ¼ncelleme hatasÄ±: {e}")
        return False


def process_single_file(driver, json_path, args):
    """Tek bir JSON dosyasÄ±nÄ± iÅŸler"""
    print(f"\n{'='*70}")
    print(f"ğŸ“‚ Dosya: {json_path.name}")
    print(f"{'='*70}")

    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"âŒ JSON okuma hatasÄ±: {e}")
        return False

    linkedin_url = data.get('social', {}).get('linkedin', '')
    if not linkedin_url:
        print("âš ï¸  LinkedIn linki bulunamadÄ±")
        return False

    print(f"ğŸ”— LinkedIn: {linkedin_url}")

    # Mevcut bilgileri gÃ¶ster
    print(f"\nğŸ“Š Mevcut Bilgiler:")
    print(f"  Name: {data.get('name', '(boÅŸ)')}")
    print(f"  Tagline: {data.get('tagline', '(boÅŸ)')}")
    print(f"  About: {data.get('about', '(boÅŸ)')[:100]}...")

    # LinkedIn'den bilgi Ã§ek
    linkedin_info = extract_linkedin_info_from_page(driver, linkedin_url, timeout=args.timeout)

    if not linkedin_info:
        print("âŒ LinkedIn bilgileri Ã§ekilemedi")
        return False

    # JSON'u gÃ¼ncelle
    success = update_company_json_with_linkedin(json_path, linkedin_info, dry_run=args.dry_run, force=args.force)

    return success


def process_all_files(driver, args):
    """TÃ¼m company JSON dosyalarÄ±nÄ± iÅŸler"""
    company_dir = Path('public/data/company')

    if not company_dir.exists():
        print(f"âŒ KlasÃ¶r bulunamadÄ±: {company_dir}")
        return

    json_files = sorted(company_dir.glob('*.json'))

    if not json_files:
        print(f"âŒ JSON dosyasÄ± bulunamadÄ±")
        return

    print(f"ğŸ“ Toplam {len(json_files)} dosya bulundu")

    if args.limit:
        json_files = json_files[:args.limit]
        print(f"âš ï¸  Limit: Ä°lk {args.limit} dosya iÅŸlenecek")

    stats = {
        'total': len(json_files),
        'success': 0,
        'failed': 0,
        'skipped': 0
    }

    for i, json_path in enumerate(json_files, 1):
        print(f"\n{'='*70}")
        print(f"[{i}/{len(json_files)}] Ä°ÅŸleniyor...")
        print(f"{'='*70}")

        try:
            success = process_single_file(driver, json_path, args)
            if success:
                stats['success'] += 1
            else:
                stats['skipped'] += 1
        except Exception as e:
            print(f"âŒ Hata: {e}")
            stats['failed'] += 1

        if i < len(json_files):
            wait_time = args.delay
            print(f"\nâ³ {wait_time} saniye bekleniyor...")
            time.sleep(wait_time)

    print(f"\n{'='*70}")
    print(f"ğŸ“Š Ä°ÅLEM Ã–ZETÄ°")
    print(f"{'='*70}")
    print(f"Toplam: {stats['total']}")
    print(f"BaÅŸarÄ±lÄ±: {stats['success']}")
    print(f"AtlandÄ±: {stats['skipped']}")
    print(f"HatalÄ±: {stats['failed']}")
    print(f"{'='*70}")


def main():
    parser = argparse.ArgumentParser(description='LinkedIn firma bilgilerini Ã§eker (Remote Debugging)')
    parser.add_argument('json_file', nargs='?', help='Ä°ÅŸlenecek JSON dosyasÄ±')
    parser.add_argument('--all', action='store_true', help='TÃ¼m company JSON dosyalarÄ±nÄ± iÅŸle')
    parser.add_argument('--limit', type=int, help='Ä°ÅŸlenecek maksimum dosya sayÄ±sÄ±')
    parser.add_argument('--dry-run', action='store_true', help='Sadece gÃ¶ster, dosyayÄ± gÃ¼ncelleme')
    parser.add_argument('--force', action='store_true', help='Mevcut bilgilerin Ã¼zerine yaz')
    parser.add_argument('--timeout', type=int, default=30, help='Sayfa yÃ¼kleme timeout')
    parser.add_argument('--delay', type=int, default=7, help='Dosyalar arasÄ± bekleme (saniye)')
    parser.add_argument('--debug-port', default='localhost:9222', help='Remote debugging address')

    args = parser.parse_args()

    # Remote tarayÄ±cÄ±ya baÄŸlan
    driver = connect_to_remote_browser(args.debug_port)
    if not driver:
        return 1

    try:
        if args.all or not args.json_file:
            process_all_files(driver, args)
        else:
            json_path = Path(args.json_file)
            if not json_path.exists():
                alt_path = Path('public/data/company') / json_path.name
                if alt_path.exists():
                    json_path = alt_path
                else:
                    print(f"âŒ Dosya bulunamadÄ±: {args.json_file}")
                    return 1

            success = process_single_file(driver, json_path, args)
            return 0 if success else 1
    finally:
        # Driver'Ä± kapatma - aÃ§Ä±k tarayÄ±cÄ±ya baÄŸlandÄ±ÄŸÄ±mÄ±z iÃ§in kapatmÄ±yoruz
        print("\nâœ… Ä°ÅŸlem tamamlandÄ±!")

    return 0


if __name__ == '__main__':
    sys.exit(main())
