#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LinkedIn Firma Bilgisi Ã‡ekme Script
LinkedIn linklerinden firma adÄ± ve hakkÄ±nda bilgisini Ã§eker.
"""

import json
import re
import sys
import time
from pathlib import Path
from urllib.parse import urlparse
import requests
from bs4 import BeautifulSoup

# Windows terminal iÃ§in encoding ayarÄ±
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')


def fetch_linkedin_html(linkedin_url, timeout=30):
    """LinkedIn sayfasÄ±ndan HTML iÃ§eriÄŸini Ã§eker"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
        }

        # SSL uyarÄ±sÄ±nÄ± bastÄ±r
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

        session = requests.Session()
        session.headers.update(headers)

        response = session.get(linkedin_url, timeout=timeout, allow_redirects=True, verify=False)
        response.encoding = 'utf-8'

        if response.status_code == 200:
            return response.text
        else:
            print(f"âš ï¸  HTTP {response.status_code}")
            return None

    except requests.RequestException as e:
        print(f"âŒ LinkedIn HTML Ã§ekme hatasÄ±: {e}")
        return None


def clean_linkedin_text(text):
    """LinkedIn'den gelen metni temizler"""
    import html
    if not text:
        return ''

    # HTML entity'leri decode et
    text = html.unescape(text)

    # "Company Name | LinkedIn'de 123" formatÄ±nÄ± temizle
    if ' | LinkedIn' in text:
        # LinkedIn prefix'i kaldÄ±r
        parts = text.split(' | ')
        # Ä°lk part firma bilgisi, son part LinkedIn meta
        if len(parts) >= 2:
            # "Company Name" kÄ±smÄ±nÄ± al
            text = parts[0].strip()

    return text.strip()


def extract_linkedin_info(html):
    """HTML'den LinkedIn firma bilgilerini Ã§Ä±karÄ±r"""
    soup = BeautifulSoup(html, 'html.parser')

    info = {
        'name': '',
        'about': '',
        'tagline': '',
        'industry': '',
        'company_size': '',
        'headquarters': '',
        'founded': ''
    }

    try:
        # Firma adÄ± - meta tag'den
        og_title = soup.find('meta', property='og:title')
        if og_title and og_title.get('content'):
            # "Company Name | LinkedIn" formatÄ±ndan sadece firma adÄ±nÄ± al
            title = og_title.get('content', '')
            info['name'] = clean_linkedin_text(title.split('|')[0])

        # Alternatif: title tag'den
        if not info['name']:
            title_tag = soup.find('title')
            if title_tag:
                title = title_tag.text
                info['name'] = clean_linkedin_text(title.split('|')[0])

        # HakkÄ±nda (about) bilgisi - meta description'dan
        og_description = soup.find('meta', property='og:description')
        if og_description and og_description.get('content'):
            raw_about = og_description.get('content', '').strip()
            raw_about = clean_linkedin_text(raw_about)

            # LinkedIn meta formatÄ±nÄ± temizle
            # Format: "Company Name | LinkedIn'de 123 takipÃ§i Actual description here"
            # veya: "LinkedIn'de 123 takipÃ§i Actual description here"

            # "LinkedIn'de X takipÃ§i/followers" kÄ±smÄ±nÄ± kaldÄ±r
            raw_about = re.sub(r"^.*?\s*\|\s*LinkedIn'de\s+[\d\.,]+\s+takipÃ§i\s+", '', raw_about)
            raw_about = re.sub(r"^.*?\s*\|\s*LinkedIn'de\s+[\d\.,]+\s+followers?\s+", '', raw_about)
            raw_about = re.sub(r"^LinkedIn'de\s+[\d\.,]+\s+takipÃ§i\s+", '', raw_about)
            raw_about = re.sub(r"^LinkedIn'de\s+[\d\.,]+\s+followers?\s+", '', raw_about)

            info['about'] = raw_about.strip()

        # Alternatif: meta description tag'den
        if not info['about']:
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc and meta_desc.get('content'):
                raw_desc = meta_desc.get('content', '').strip()
                raw_desc = clean_linkedin_text(raw_desc)
                raw_desc = re.sub(r"^.*?\s*\|\s*LinkedIn'de\s+[\d\.,]+\s+takipÃ§i\s+", '', raw_desc)
                raw_desc = re.sub(r"^LinkedIn'de\s+[\d\.,]+\s+takipÃ§i\s+", '', raw_desc)
                info['about'] = raw_desc.strip()

        # Tagline - about'tan ilk anlamlÄ± cÃ¼mleyi Ã§Ä±kar
        if info['about'] and len(info['about']) > 20:
            # Ä°lk cÃ¼mleyi veya ilk pipe'a kadar olan kÄ±smÄ± al
            if '|' in info['about']:
                tagline = info['about'].split('|')[0].strip()
            else:
                # Ä°lk cÃ¼mleyi al (. veya ! veya ?)
                sentences = re.split(r'[.!?]\s+', info['about'])
                tagline = sentences[0].strip() if sentences else ''

            # Makul bir tagline uzunluÄŸu kontrolÃ¼
            if 10 < len(tagline) < 200:
                info['tagline'] = tagline

        # JSON-LD structured data'dan bilgi Ã§ek
        json_ld_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_ld_scripts:
            try:
                data = json.loads(script.string)
                if isinstance(data, dict):
                    if data.get('@type') == 'Organization':
                        if not info['name'] and data.get('name'):
                            info['name'] = data['name']
                        if not info['about'] and data.get('description'):
                            info['about'] = data['description']
            except:
                continue

        # Sayfa iÃ§eriÄŸinden detaylÄ± bilgi Ã§ekme (JavaScript render gerektirmeden)
        # LinkedIn'in public gÃ¶rÃ¼nÃ¼mÃ¼nde bazÄ± bilgiler meta tag'lerde olabilir

        # Industry bilgisi
        for text in soup.stripped_strings:
            if 'Industry' in text or 'SektÃ¶r' in text:
                # Sonraki text node'u industry olabilir
                pass

        # Company size
        for text in soup.stripped_strings:
            if 'employees' in text.lower() or 'Ã§alÄ±ÅŸan' in text.lower():
                # Size bilgisi
                match = re.search(r'(\d+[\-,]\d+|\d+\+?)\s*(employees|Ã§alÄ±ÅŸan)', text, re.IGNORECASE)
                if match:
                    info['company_size'] = match.group(1)

    except Exception as e:
        print(f"âš ï¸  Bilgi Ã§Ä±karma hatasÄ±: {e}")

    return info


def update_company_json_with_linkedin(json_path, linkedin_info, dry_run=False, force=False):
    """JSON dosyasÄ±nÄ± LinkedIn bilgileriyle gÃ¼nceller"""
    try:
        # JSON dosyasÄ±nÄ± oku
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # Mevcut deÄŸerleri sakla
        old_name = data.get('name', '')
        old_about = data.get('about', '')
        old_tagline = data.get('tagline', '')

        updated = False
        updates = []

        # Name gÃ¼ncelle (sadece boÅŸsa veya force modundaysa)
        if linkedin_info['name'] and (not old_name or force):
            data['name'] = linkedin_info['name']
            updated = True
            updates.append(f"Name: {linkedin_info['name']}")

        # About gÃ¼ncelle (sadece boÅŸsa veya force modundaysa)
        if linkedin_info['about'] and (not old_about or force):
            data['about'] = linkedin_info['about']
            updated = True
            updates.append(f"About: {linkedin_info['about'][:100]}...")

        # Tagline gÃ¼ncelle (sadece boÅŸsa veya force modundaysa)
        if linkedin_info['tagline'] and (not old_tagline or force):
            data['tagline'] = linkedin_info['tagline']
            updated = True
            updates.append(f"Tagline: {linkedin_info['tagline']}")

        if not updated:
            print("  â„¹ï¸  GÃ¼ncellenecek yeni bilgi bulunamadÄ± (zaten dolu veya LinkedIn'den bilgi alÄ±namadÄ±)")
            return False

        # GÃ¼ncellemeleri gÃ¶ster
        print("\nğŸ“ GÃ¼ncellenecek alanlar:")
        for update in updates:
            print(f"  â€¢ {update}")

        # DosyayÄ± kaydet
        if not dry_run:
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"\nâœ… Dosya gÃ¼ncellendi: {json_path}")
        else:
            print(f"\nğŸ” DRY RUN: Dosya gÃ¼ncellenmedi (--dry-run)")

        return True

    except Exception as e:
        print(f"âŒ JSON gÃ¼ncelleme hatasÄ±: {e}")
        return False


def process_single_file(json_path, args):
    """Tek bir JSON dosyasÄ±nÄ± iÅŸler"""
    print(f"\n{'='*70}")
    print(f"ğŸ“‚ Dosya: {json_path.name}")
    print(f"{'='*70}")

    # JSON dosyasÄ±nÄ± oku
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"âŒ JSON okuma hatasÄ±: {e}")
        return False

    # LinkedIn linkini al
    linkedin_url = data.get('social', {}).get('linkedin', '')
    if not linkedin_url:
        print("âš ï¸  LinkedIn linki bulunamadÄ±")
        return False

    print(f"ğŸ”— LinkedIn: {linkedin_url}")

    # Mevcut bilgileri gÃ¶ster
    print(f"\nğŸ“Š Mevcut Bilgiler:")
    print(f"  Name: {data.get('name', '(boÅŸ)')}")
    print(f"  Tagline: {data.get('tagline', '(boÅŸ)')}")
    print(f"  About: {data.get('about', '(boÅŸ)')[:100]}{'...' if len(data.get('about', '')) > 100 else ''}")

    # LinkedIn'den bilgi Ã§ek
    print(f"\nğŸ” LinkedIn bilgileri Ã§ekiliyor...")
    html = fetch_linkedin_html(linkedin_url, timeout=args.timeout)

    if not html:
        print("âŒ LinkedIn HTML Ã§ekilemedi")
        return False

    # Bilgileri Ã§Ä±kar
    linkedin_info = extract_linkedin_info(html)

    print(f"\nâœ… LinkedIn'den Ã§ekilen bilgiler:")
    print(f"  Name: {linkedin_info['name'] or '(bulunamadÄ±)'}")
    print(f"  Tagline: {linkedin_info['tagline'] or '(bulunamadÄ±)'}")
    print(f"  About: {linkedin_info['about'][:100] if linkedin_info['about'] else '(bulunamadÄ±)'}{'...' if len(linkedin_info['about']) > 100 else ''}")

    # JSON'u gÃ¼ncelle
    success = update_company_json_with_linkedin(json_path, linkedin_info, dry_run=args.dry_run, force=args.force)

    return success


def process_all_files(args):
    """TÃ¼m company JSON dosyalarÄ±nÄ± iÅŸler"""
    company_dir = Path('public/data/company')

    if not company_dir.exists():
        print(f"âŒ KlasÃ¶r bulunamadÄ±: {company_dir}")
        return

    # TÃ¼m JSON dosyalarÄ±nÄ± al
    json_files = sorted(company_dir.glob('*.json'))

    if not json_files:
        print(f"âŒ JSON dosyasÄ± bulunamadÄ±: {company_dir}")
        return

    print(f"ğŸ“ Toplam {len(json_files)} dosya bulundu")

    if args.limit:
        json_files = json_files[:args.limit]
        print(f"âš ï¸  Limit uygulandÄ±: Ä°lk {args.limit} dosya iÅŸlenecek")

    # Ä°statistikler
    stats = {
        'total': len(json_files),
        'success': 0,
        'failed': 0,
        'skipped': 0
    }

    # Her dosyayÄ± iÅŸle
    for i, json_path in enumerate(json_files, 1):
        print(f"\n{'='*70}")
        print(f"[{i}/{len(json_files)}] Ä°ÅŸleniyor...")
        print(f"{'='*70}")

        try:
            success = process_single_file(json_path, args)
            if success:
                stats['success'] += 1
            else:
                stats['skipped'] += 1
        except Exception as e:
            print(f"âŒ Hata: {e}")
            stats['failed'] += 1

        # Rate limiting - LinkedIn'in throttle yapmamasÄ± iÃ§in
        if i < len(json_files):
            wait_time = args.delay
            print(f"\nâ³ {wait_time} saniye bekleniyor...")
            time.sleep(wait_time)

    # Ã–zet
    print(f"\n{'='*70}")
    print(f"ğŸ“Š Ä°ÅLEM Ã–ZETÄ°")
    print(f"{'='*70}")
    print(f"Toplam: {stats['total']}")
    print(f"BaÅŸarÄ±lÄ±: {stats['success']}")
    print(f"AtlandÄ±: {stats['skipped']}")
    print(f"HatalÄ±: {stats['failed']}")
    print(f"{'='*70}")


def main():
    import argparse

    parser = argparse.ArgumentParser(description='LinkedIn firma bilgilerini Ã§eker ve JSON dosyalarÄ±nÄ± gÃ¼nceller')
    parser.add_argument('json_file', nargs='?', help='Ä°ÅŸlenecek JSON dosyasÄ± (belirtilmezse tÃ¼m dosyalar iÅŸlenir)')
    parser.add_argument('--all', action='store_true', help='TÃ¼m company JSON dosyalarÄ±nÄ± iÅŸle')
    parser.add_argument('--limit', type=int, help='Ä°ÅŸlenecek maksimum dosya sayÄ±sÄ±')
    parser.add_argument('--dry-run', action='store_true', help='Sadece gÃ¶ster, dosyayÄ± gÃ¼ncelleme')
    parser.add_argument('--force', action='store_true', help='Mevcut bilgilerin Ã¼zerine yaz')
    parser.add_argument('--timeout', type=int, default=30, help='HTTP timeout (saniye, varsayÄ±lan: 30)')
    parser.add_argument('--delay', type=int, default=5, help='Dosyalar arasÄ± bekleme sÃ¼resi (saniye, varsayÄ±lan: 5)')

    args = parser.parse_args()

    # TÃ¼m dosyalarÄ± iÅŸle
    if args.all or not args.json_file:
        process_all_files(args)
        return 0

    # Tek dosya iÅŸle
    json_path = Path(args.json_file)
    if not json_path.exists():
        # public/data/company/ iÃ§inde ara
        alt_path = Path('public/data/company') / json_path.name
        if alt_path.exists():
            json_path = alt_path
        else:
            print(f"âŒ Dosya bulunamadÄ±: {args.json_file}")
            return 1

    success = process_single_file(json_path, args)
    return 0 if success else 1


if __name__ == '__main__':
    sys.exit(main())
